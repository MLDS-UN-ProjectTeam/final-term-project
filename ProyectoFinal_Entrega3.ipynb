{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": "<img src = \"./resources/images/header_banner_3.jpeg\" alt = \"Encabezado MLDS\" width = \"100%\">  </img>",
   "metadata": {
    "id": "2qrgS_sCJquS"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fb006272"
   },
   "source": [
    "# **Preparación de los datos**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fdb4c33d"
   },
   "source": [
    "## **0. Integrantes del equipo de trabajo**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ckNDI7ftUrpa"
   },
   "source": [
    "<table><thead>\n",
    "  <tr>\n",
    "    <th>#</th>\n",
    "    <th>Integrante</th>\n",
    "    <th>Documento de identidad</th>\n",
    "  </tr></thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td>1</td>\n",
    "    <td>Laura Alejandra Díaz López</td>\n",
    "    <td>1010018062</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>2</td>\n",
    "    <td>Diego Alejandro Feliciano Ramos</td>\n",
    "    <td>1024586904</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>3</td>\n",
    "    <td>Geraldine Gracia Ruiz</td>\n",
    "    <td>1032488268</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56fe78e6"
   },
   "source": [
    "## **1. Limpieza de los Datos**\n",
    "---\n",
    "\n",
    "La elección de las técnicas de preprocesamiento puede diferir en cada conjunto de datos. Recuerde que es posible aplicar, según sea necesario (no necesariamente todas), las técnicas generales que se han explorado en el curso. La elección dependerá del tipo de datos con el que esté trabajando.\n",
    "\n",
    "A lo largo de esta entrega, busque responder las siguientes preguntas:\n",
    "\n",
    "- ¿Cuáles fueron los criterios utilizados para identificar y tratar valores atípicos, datos faltantes o cualquier otra anomalía en el conjunto de datos durante el proceso de limpieza?\n",
    "- ¿Cómo se justificaría la necesidad de cada paso de preprocesamiento en términos de mejora de la calidad de los datos y preparación para el análisis subsiguiente?\n",
    "\n",
    "A continuación encontrará los puntos a tratar a medida que va realizando la preparación de los datos. En cada punto defina el estado en que se encontraba el dataset, ademas de explicar y justificar las acciones y decisiones que se tomaron."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Instalamos los paquetes necesarios para el notebook en caso de trabajar desde Google Colab\n",
    "!pip install wget pandas ydata-profiling ipywidgets"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import wget, pandas as pd\n",
    "\n",
    "# Descargar el dataset transformado de la entrega anterior del repositorio publico\n",
    "url = 'https://github.com/MLDS-UN-ProjectTeam/final-term-project/raw/main/resources/data/Saber_11_2023-2_-_Transformado.csv?download='\n",
    "downloaded_file_name = 'Saber_11_2023-2_-_Transformado.csv'\n",
    "wget.download(url = url, out = downloaded_file_name)\n",
    "\n",
    "# Cargar en el notebook el conjunto de datos con las restricciones conocidas:\n",
    "# - Delimitador: ¬ \n",
    "# - Quoting = 3 => Sin comillas para todas las variables\n",
    "saber11_dataframe = pd.read_csv(downloaded_file_name, delimiter=',', quoting=3)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **1.1. Valores faltantes**\n",
    "---\n",
    "Al encontrarnos con valores faltantes en el conjunto de datos, es crucial preguntarse:\n",
    "1. **¿Cómo afectan estos valores a la integridad y representatividad de la información?**\n",
    "\n",
    "La ausencia o falta de datos en un conjunto puede impactar negativamente tanto la calidad como la precisión de la información analizada. Esto ocurre de dos maneras principales:\n",
    "* En cuanto a la calidad de los datos:\n",
    "    * Los análisis pueden ser imprecisos debido a cálculos erróneos.\n",
    "    * Se puede perder información valiosa, limitando la utilidad del análisis.\n",
    "    * Los modelos predictivos pueden fallar o funcionar de manera deficiente.\n",
    "* Respecto a la precisión de los datos:\n",
    "    * Pueden surgir sesgos si la falta de datos no es aleatoria.\n",
    "    * Por ejemplo, en estudios médicos, si los pacientes más graves tienen más datos faltantes, se podría subestimar la severidad de la enfermedad.\n",
    "    * La capacidad de detectar patrones significativos se reduce al haber menos datos disponibles.\n",
    "Para abordar estos problemas, existen varias estrategias:\n",
    "* Eliminar registros incompletos, aunque esto puede resultar en pérdida de información.\n",
    "* Estimar los valores faltantes, pero esto puede introducir suposiciones inexactas.\n",
    "* Utilizar algoritmos diseñados para manejar datos incompletos.\n",
    "\n",
    "2. **¿Cómo se identificaron los valores faltantes en el conjunto de datos?**"
   ],
   "metadata": {}
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Para la identificación de valores faltantes usamos pandas y varias de las funciones que nos ofrece su api, aquí será descrito cada método que se usó y la finalidad con la que se usó\n",
    "\n",
    "# Contar el total de valores faltantes en el DataFrame\n",
    "total_faltantes = saber11_dataframe.isnull().sum().sum()\n",
    "print('\\nTotal faltantes = ' + str(total_faltantes))\n",
    "\n",
    "#Cantidad de datos nulos por columna\n",
    "columnas_con_faltantes = saber11_dataframe.isnull().sum()\n",
    "columnas_con_faltantes = columnas_con_faltantes[columnas_con_faltantes != 0]\n",
    "print(columnas_con_faltantes)\n",
    "\n",
    "# Rellenamos columnas de interés con la media de las ocurrencias\n",
    "media_estrato_vivienda = saber11_dataframe['FAMI_ESTRATOVIVIENDA_N'].mean()\n",
    "saber11_dataframe['FAMI_ESTRATOVIVIENDA_N'] = saber11_dataframe['FAMI_ESTRATOVIVIENDA_N'].fillna(media_estrato_vivienda)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "f4K7dEZKeiit"
   },
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "3. **¿Cuáles fueron los criterios para decidir si rellenar con valores estimados o eliminar los valores faltantes? En caso que aplique, ¿qué método de relleno se utilizó y por qué se consideró apropiado?**\n",
    "\n",
    "El criterio que se usa y usará durante esta entrega será el de que sea una variable numérica, no categórica, ya que nos permite ver la distribución de los datos, así como que no tenga una gran cantidad de datos faltantes, estimando esta gran cantidad en más del 10% de los datos ~ > 55100 datos por columna como máximo. lo cuál solo ocurre para una variable de interés (`FAMI_ESTRATOVIVIENDA_N`). "
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **1.2. Valores duplicados**\n",
    "----\n",
    "En el análisis del conjunto de datos, se comprobó que no existen valores duplicados. El conjunto de datos no presenta duplicados. La variable *ESTU_CONSECUTIVO*, que corresponde al ID público del inscrito (SB11) fue estudiada y se verificó que la cantidad de valores únicos en esta columna coincide con el total de registros del conjunto de datos. Esto confirma que cada registro representa a un inscrito diferente.\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "4qKkkYiCeqFg"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "--J8K7mfgm2m"
   },
   "source": [
    "print(f\"Cantidad de filas en el conjunto de datos: {saber11_dataframe.shape[0]}\")\n",
    "print(f\"Cantidad de valores únicos en la variable ESTU_CONSECUTIVO: {saber11_dataframe['ESTU_CONSECUTIVO'].nunique()}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **1.3. Valores atípicos**\n",
    "---\n",
    "Al abordar valores atípicos, es relevante cuestionarse sobre la naturaleza de estos puntos extremos.\n",
    "* ¿Son errores de medición o representan información válida pero excepcional?\n",
    "\n",
    "En nuestro caso representan información valida pero excepcional, esto se ve reflejado particularmente en las variables que han sido transformadas, como en el caso de la cantidad de libros leídos por familia(`FAMI_NUMLIBROS_N`) y la correlación directa con la variable objetivo a estimar (`PUNT_GLOBAL`) \n",
    "\n",
    "* ¿Qué criterios o técnicas se utilizaron para identificar los valores atípicos?\n",
    "* ¿Se aplicaron métodos estadísticos o visuales para detectar los valores atípicos?\n",
    "\n",
    "Para responder a las anteriores preguntas se usó la técnica descrita en la próxima celda de código, donde nos apoyamos en los conceptos estadísticos de cuantiles y rango intercuartilico con las variables numéricas para hallar los valores atípicos; gracias a esto también llegamos a la conclusión anterior. En la que establecemos que es información válida pero excepcional al analizar los, aproximadamente, 186000 registros, esto es, aproximadamente un 33% del conjunto de datos.\n",
    "\n",
    "Aquí vemos que la cantidad de outliers en realidad parece ser dada cuando no hay datos disponibles en alguna de las columnas numéricas, tomando como fuente de verdad la cantidad de NaN en el mismo dataframe retornado por la función   "
   ],
   "metadata": {}
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Función para identificar outliers usando el rango intercuartilico\n",
    "def identificar_outliers_iqr(df, multiplicador = 1.5):\n",
    "    outliers = pd.DataFrame()\n",
    "    \n",
    "    for col in saber11_dataframe.select_dtypes(include='number').columns:  # Solo columnas numéricas\n",
    "        q1 = df[col].quantile(0.25)\n",
    "        q3 = df[col].quantile(0.75)\n",
    "        rqi = q3 - q1\n",
    "\n",
    "        limite_inferior = q1 - (multiplicador * rqi)\n",
    "        limite_superior = q3 + (multiplicador * rqi)\n",
    "        \n",
    "        # Filtrar outliers para cada columna\n",
    "        outliers_col = df[(df[col] < limite_inferior) | (df[col] > limite_superior)]\n",
    "        \n",
    "        # Concatenar resultados\n",
    "        outliers = pd.concat([outliers, outliers_col])\n",
    "    \n",
    "    return outliers.drop_duplicates()\n",
    "\n",
    "# Identificamos outliers en el DataFrame completo\n",
    "outliers = identificar_outliers_iqr(saber11_dataframe)\n",
    "print(outliers)\n",
    "\n",
    "# Identificamos las columnas con outliers\n",
    "filtro_outliers = outliers.isna().sum() != 0\n",
    "print(outliers.isnull().sum()[filtro_outliers])\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "4pVqw4XWeuly"
   },
   "cell_type": "markdown",
   "source": [
    "\n",
    "* ¿Cuál fue la decisión final sobre cómo tratar los valores atípicos y por qué?\n",
    "\n",
    "\n",
    "Debido al impacto que tienen estos valores atípicos, afectando al 33% del conjunto de datos, se ha decidido:\n",
    "* En primer lugar, ampliar la ventana de elección de rangos inferiores y superiores para las variables numéricas para obtener mayor cantidad de datos ya que estas ocurrencias son, en su mayoría, medidas excepcionales sobre la variable tomada\n",
    "* En segundo lugar, eliminar el resto de ocurrencias con la nueva aproximación tomada. Esto se puede ver en la siguiente celda de código.  "
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Identificamos los nuevos outliers en el DataFrame completo\n",
    "outliers = identificar_outliers_iqr(saber11_dataframe, 2)\n",
    "print(outliers)\n",
    "\n",
    "# Identificamos las columnas con outliers\n",
    "filtro_outliers = outliers.isna().sum() != 0\n",
    "print(outliers.isnull().sum()[filtro_outliers])\n",
    "\n",
    "# Convertir filas a tuplas para comparación\n",
    "tuplas_saber11_dataframe = saber11_dataframe.apply(tuple, axis=1)\n",
    "tuplas_outliers = outliers.apply(tuple, axis=1)\n",
    "\n",
    "# Eliminar filas en saber_11_dataframe que son outliers\n",
    "saber11_dataframe_no_outliers = saber11_dataframe[~tuplas_saber11_dataframe.isin(tuplas_outliers)]\n",
    "saber11_dataframe_no_outliers.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **1.4. Datos Inconsistentes**\n",
    "---\n",
    "\n",
    "El conjunto de datos no presenta inconsistencias en su mayoría. Sin embargo, a partir del análisis realizado en la fase anterior, se identificó una inconsistencia en la variable `COLE_SEDE_PRINCIPAL`, que contiene la respuesta a la pregunta: ¿Esta es la sede principal del Establecimiento Educativo?\n",
    "\n",
    "De acuerdo con la documentación, las opciones de respuesta son: 'S' o 'N'. No obstante, se observaron cuatro valores únicos en esta columna, lo cual se debe a que en algunas filas la respuesta tiene un espacio adicional al final, quedando como 'S ' y 'N '.\n",
    "\n",
    "<img src = \"./resources/images/COLE_SEDE_PRINCIPAL_inconsistencia.jpg\" alt = \"COLE_SEDE_PRINCIPAL\" width = \"100%\">  </img>\n",
    "\n",
    "Aunque la variable no es de interés para objetivo del análisis, esta inconsistencia se corregirá aplicando el método `strip()` a los valores de la columna, lo que eliminará el espacio adicional y asegurará la consistencia de los datos."
   ],
   "metadata": {
    "id": "VIsvbU-ueysh"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "V2ikNtJcgn8C"
   },
   "source": [
    "# Valores únicos en la columna COLE_SEDE_PRINCIPAL original\n",
    "saber11_dataframe_no_outliers['COLE_SEDE_PRINCIPAL'].unique() \n",
    "\n",
    "# Se eliminan espacios adicionales con el método strip()\n",
    "saber11_dataframe_no_outliers['COLE_SEDE_PRINCIPAL'] = saber11_dataframe_no_outliers['COLE_SEDE_PRINCIPAL'].str.strip()\n",
    "\n",
    "# Valores únicos en la columna COLE_SEDE_PRINCIPAL después de corregir\n",
    "saber11_dataframe_no_outliers['COLE_SEDE_PRINCIPAL'].unique() "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **1.5. Datos corruptos**\n",
    "---\n",
    "\n",
    "En la exploración del conjunto de datos, no se evidencian datos corruptos. En este caso, la información de los inscrito, al ser recolectada a través de una encuesta con preguntas directas y opciones de respuesta predefinidas, se limita la existencia de errores de entrada."
   ],
   "metadata": {
    "id": "i2HDxOgae2zH"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **1.6. Selección de datos**\n",
    "---\n",
    "En la selección de datos, reflexione sobre las variables que son realmente relevantes para sus objetivos de análisis.\n",
    "* ¿Qué criterios se utilizaron para seleccionar los datos relevantes para el análisis?\n",
    "* ¿Se aplicaron técnicas de muestreo o filtrado para reducir el tamaño del conjunto de datos?\n",
    "* ¿Cómo se justificó la inclusión o exclusión de ciertas variables en la selección de datos?"
   ],
   "metadata": {
    "id": "ciaXdOS5fY6a"
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Para reducir el tamaño del conjunto de datos el recurso principal fue enfocar el estudio y análisis solamente a Bogotá D.C, en vez de hacerlo a nivel nacional, lo cual está en concordancia con un tipo de muestreo no probabilístico llamado muestreo intencional. Se consideró la amplia población de la ciudad como una buena muestra dado que en la capital convergen todo tipo de estratos socioeconómicos, experiencias de vida y, adicionalmente, muchos jovenes son obligados a desplazarse a ella a lo largo de su adolescencia, por lo que hacia ella confluyen historias de todo el país. \n",
    "\n",
    "Ahora, con mayor enfoque hacia la selección de datos: el criterio principal fue el objetivo del estudio, encontrar relaciones importantes entre variables de indicadores socioeconómicos y el resultado del examen de estado. Esta filtración de datos se ha venido haciendo desde la entrega anterior, pues ha sido fácil excluir variables que no proveían información de interés para lo que compete a este análisis. Adicional a esto, al evaluar la calidad de los datos, las variables con un alto porcentaje de valores nulos también decidieron descartarse dado su aporte turbulento al dataset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "aKwsi2fmgooI"
   },
   "source": [
    "from ydata_profiling import ProfileReport\n",
    "# De acuerdo con el profiling y el trabajo realizado en entregas anteriores se eliminarán columnas que tienen alguna de las siguientes características:\n",
    "# - Tienen todos sus valores como ocurrencias únicas\n",
    "# - Son constantes\n",
    "# - Tienen alta correlación\n",
    "# - Tienen una gran cantidad (>10%) de datos faltantes\n",
    "# - No son útiles a nuestro entender para la intención que se tiene con el set de datos\n",
    "# - Tienen un desbalanceo muy fuerte, donde el 90% o una mayor proporción de las ocurrencias está en una sola categoría/etiqueta para la columna eliminada\n",
    "# - Se explican gracias a otras (factores socioeconómicos con nivel socioeconómico, por ejemplo) \n",
    "columnas_a_eliminar = [\n",
    "    'ESTU_TIPODOCUMENTO',               #\n",
    "    'ESTU_NACIONALIDAD',                #\n",
    "    'ESTU_FECHANACIMIENTO',             #\n",
    "    'PERIODO',                          #\n",
    "    'ESTU_CONSECUTIVO',                 #\n",
    "    'ESTU_ESTUDIANTE',                  #\n",
    "    'ESTU_PAIS_RESIDE',                 #\n",
    "    'ESTU_DEPTO_RESIDE',                #\n",
    "    'ESTU_MCPIO_RESIDE',                #\n",
    "    'ESTU_COD_RESIDE_DEPTO',            #\n",
    "    'ESTU_COD_RESIDE_MCPIO',            #\n",
    "    'ESTU_PRESENTACIONSABADO',          #\n",
    "    'ESTU_LENGUANATIVA',                #\n",
    "    'FAMI_ESTRATOVIVIENDA',             #\n",
    "    'FAMI_TIENEAUTOMOVIL',              #\n",
    "    'FAMI_TIENECOMPUTADOR',             #\n",
    "    'FAMI_TIENEHORNOMICROOGAS',         #\n",
    "    'FAMI_TIENEINTERNET',               #\n",
    "    'FAMI_TIENELAVADORA',               #\n",
    "    'FAMI_PERSONASHOGAR',               #\n",
    "    'FAMI_CUARTOSHOGAR',                #\n",
    "    'FAMI_TIENESERVICIOTV',             #\n",
    "    'FAMI_TIENEMOTOCICLETA',            #\n",
    "    'FAMI_TIENECONSOLAVIDEOJUEGOS',     #\n",
    "    'FAMI_NUMLIBROS',                   #\n",
    "    'FAMI_COMELECHEDERIVADOS',          #\n",
    "    'FAMI_COMECARNEPESCADOHUEVO',       #\n",
    "    'FAMI_COMECEREALFRUTOSLEGUMBRE',    #\n",
    "    'FAMI_SITUACIONECONOMICA',          #\n",
    "    'COLE_AREA_UBICACION',              #\n",
    "    'COLE_MCPIO_UBICACION',             #\n",
    "    'COLE_COD_MCPIO_UBICACION',         #\n",
    "    'COLE_COD_DEPTO_UBICACION',         #\n",
    "    'COLE_COD_DANE_SEDE',               #\n",
    "    'COLE_NOMBRE_SEDE',                 #\n",
    "    'COLE_CALENDARIO',                  #\n",
    "    'COLE_BILINGUE',                    #\n",
    "    'COLE_GENERO',                      #\n",
    "    'COLE_CODIGO_ICFES',                #\n",
    "    'COLE_COD_DANE_ESTABLECIMIENTO',    #\n",
    "    'COLE_NOMBRE_ESTABLECIMIENTO',      #\n",
    "    'COLE_SEDE_PRINCIPAL',              #\n",
    "    'COLE_DEPTO_UBICACION',             #\n",
    "    'ESTU_PRIVADO_LIBERTAD',            #\n",
    "    'ESTU_COD_MCPIO_PRESENTACION',      #\n",
    "    'ESTU_MCPIO_PRESENTACION',          #\n",
    "    'ESTU_DEPTO_PRESENTACION',          #\n",
    "    'ESTU_COD_DEPTO_PRESENTACION',      #\n",
    "    'PERCENTIL_LECTURA_CRITICA',        #\n",
    "    'DESEMP_LECTURA_CRITICA',           #\n",
    "    'PUNT_LECTURA_CRITICA',             #\n",
    "    'PERCENTIL_MATEMATICAS',            #\n",
    "    'DESEMP_MATEMATICAS',               #\n",
    "    'PUNT_MATEMATICAS',                 #\n",
    "    'PERCENTIL_C_NATURALES',            #\n",
    "    'DESEMP_C_NATURALES',               #\n",
    "    'PUNT_C_NATURALES',                 #\n",
    "    'PERCENTIL_SOCIALES_CIUDADANAS',    #\n",
    "    'DESEMP_SOCIALES_CIUDADANAS',       #\n",
    "    'PUNT_SOCIALES_CIUDADANAS',         #\n",
    "    'PERCENTIL_INGLES',                 #\n",
    "    'DESEMP_INGLES',                    #\n",
    "    'PUNT_INGLES',                      #\n",
    "    'PERCENTIL_GLOBAL',                 #\n",
    "    'PERCENTIL_ESPECIAL_GLOBAL',        #\n",
    "    'ESTU_NSE_INDIVIDUAL',              #\n",
    "    'ESTU_ESTADOINVESTIGACION',         #\n",
    "    'ESTU_GENERACION',                  #\n",
    "    'ESTU_GENERO_N',                    #\n",
    "    'FAMI_PERSONASHOGAR_N',             #\n",
    "    'FAMI_TIENEINTERNET_N',             #\n",
    "    'FAMI_TIENECOMPUTADOR_N',           #\n",
    "    'FAMI_NUMLIBROS_N',                 #\n",
    "    'ESTU_HORASSEMANATRABAJA_N',        #\n",
    "    'ESTU_PRIVADO_LIBERTAD_N'           #\n",
    "]\n",
    "## Realizamos las consultas para seleccionar solo los estudiantes en Bogotá que también presentaron la prueba en Bogotá\n",
    "condicion_de_residencia = saber11_dataframe_no_outliers['ESTU_DEPTO_RESIDE'] =='BOGOTÁ'\n",
    "condicion_de_presentacion = saber11_dataframe_no_outliers['ESTU_DEPTO_PRESENTACION'] =='BOGOTÁ'\n",
    "\n",
    "# Seleccionamos con la condición anterior la parte de nuestro interés del set de datos, se eliminan las columnas descritas anteriormente y por último se eliminan las filas que quedaron vacías al eliminar outliers y aplicar las condiciones dadas (quedaron como NaN)\n",
    "saber11_dataframe_nuevo = saber11_dataframe_no_outliers.where(condicion_de_residencia & condicion_de_presentacion).drop(columns=columnas_a_eliminar).drop_duplicates()\n",
    "\n",
    "# Se genera el _profile_ en un archivo HTML para poder verlo de forma separada y no cargar el notebook con información innecesaria\n",
    "profile_nuevo = ProfileReport(saber11_dataframe_nuevo)\n",
    "profile_nuevo.to_file('Saber 11 2023-2-Report - Nuevo.html')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Créditos**\n",
    "* **Profesor:** [Felipe Restrepo Calle](https://dis.unal.edu.co/~ferestrepoca/)\n",
    "* **Asistentes docentes:**\n",
    "    - [Juan Sebastián Lara Ramírez](https://www.linkedin.com/in/juan-sebastian-lara-ramirez-43570a214/).\n",
    "* **Diseño de imágenes:**\n",
    "    - [Rosa Alejandra Superlano Esquibel](mailto:rsuperlano@unal.edu.co).\n",
    "* **Coordinador de virtualización:**\n",
    "    - [Edder Hernández Forero](https://www.linkedin.com/in/edder-hernandez-forero-28aa8b207/).\n",
    "    \n",
    "**Universidad Nacional de Colombia** - *Facultad de Ingeniería*"
   ],
   "metadata": {
    "id": "ceibuW1dNH9a"
   }
  }
 ]
}
